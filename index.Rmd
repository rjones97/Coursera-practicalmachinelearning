---
title: "Practical Machine Learning Course Project"
author: "Rachel Jones"
date: "January 27, 2017"
output: html_document
---

Note: Initial setup from  https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-ghPagesSetup.md has been very helpful.

# Instructions
One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants.

The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases.


## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

The training data for this project are available here:
 https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:
 https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

# Environment Setup
```{r EnvironmentSetup}
library(caret)
library(randomForest)
library(lattice)
library(ggplot2)
library(rpart)
library(rpart.plot)
```


# Data Setup
## Load Data
We want to load the data from provided sources.
```{r DataSetup.LoadData}
# Gather URLs for the training and testing data
urlTrain <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
urlTest <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

# TRAINING DATA
# If file does not exist, download and read in converting unuseful data to NAs
csvTraining <- "pml-training.csv"

if (file.exists(csvTraining)) {
        training <- read.csv(csvTraining, na.strings=c("NA","#DIV/0!",""))
} else { 
        download.file(urlTrain,csvTraining)
        training <- read.csv(csvTraining, na.strings=c("NA","#DIV/0!",""))
        }                           

# TESTING DATA
# Repeat the process above for Testing Data
csvTesting <-  "pml-testing.csv"

if (file.exists(csvTesting)) {
        testing <- read.csv(csvTesting, na.strings=c("NA","#DIV/0!",""))
} else { 
        download.file(urlTest,csvTesting)
        testing <- read.csv(csvTesting, na.strings=c("NA","#DIV/0!",""))
}   

# INSPECT THE DATA
#dim(training); dim(testing);
#names(training); names(testing);
#summary(training); summary(testing);
#head(training); head(testing);
#str(training); head(testing);

```

## Cleanse & Prepare Data
Now that the data is loaded, we want to cleanse it - removing unnecessary data.
```{r DataSetup.Cleanse}
# Remove any columns that contain no data
training <- training[,colSums(is.na(training)) == 0]
testing <- testing[,colSums(is.na(testing)) == 0]

# Remove columns that are unrelated to the evaluation: 
    # X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, 
    # cvtd_timestamp, new_window, num_window (columns 1-7)

training <- training[,-c(1:7)]
testing <- testing[,-c(1:7)]

```

Inspect the data.
```{r DataSetup.Inspect,results='hide',eval=FALSE}
# Inspecting the final data now that cleansing is complete
dim(training); dim(testing);
names(training); names(testing);
summary(training); summary(testing);
head(training); head(testing);
str(training); head(testing);

# Note: output has been suppressed
```



Now that the data has been cleansed, we need to prepare the data, subsetting the training data into training and testing sets; setting the provided testing set asside for final analysis.
```{r DataSetup.Prep}
set.seed(4321)
inTrain <- createDataPartition(y=training$classe, p=0.7, list = FALSE)
sub_training <- training[inTrain,]
sub_testing <- training[-inTrain,]

# Look at the 5 levels of Classe
plot(sub_training$classe, col="red", main="Frequency of CLASSE values in Training Data", xlab="classe", ylab="Frequency")
```

# Train and Predict
Because we don't know which models will perform best at predicting the outcomes, several models will be used for analysis.

## Training Models

```{r Training}
timestamp()
model_rf    <- train(classe ~., data = sub_training, method="rf")
timestamp()
model_lda   <- train(classe ~., data = sub_training, method="lda")
timestamp()
model_gbm   <- train(classe ~., data = sub_training, method="gbm")
timestamp()
model_rpart <- train(classe ~., data = sub_training, method="rpart")
timestamp()

```

```{r}
rpart.plot(model_rpart$finalModel,main="Classification Tree", extra=102, under=TRUE)
```

## Predicting Models
Using the generated models, we supply the subset of testing data to establish predictions.
```{r Predict}
predict_rf <- predict(model_rf, newdata=sub_testing)
predict_lda <- predict(model_lda, newdata=sub_testing)
predict_gbm <- predict(model_gbm, newdata=sub_testing)
predict_rpart <- predict(model_rpart, newdata=sub_testing)
```

Using the predictions generated for each of the models, we can analyze the accuracy between the varying models and identify if one model is sufficient, or if we should attempt to combine multiple models.
```{r Prediction.Accuracy}
# Random Forest (RF)
confusionMatrix(sub_testing$classe,predict_rf)

# Linear Discriminate Analysis (LDA)
confusionMatrix(sub_testing$classe,predict_lda)

# Generalized Boosted Model (GBM)
confusionMatrix(sub_testing$classe,predict_gbm)

# Decision Tree (RPart)
confusionMatrix(sub_testing$classe,predict_rpart)
```



# Summary
Comparing the results from each of the model predictions, the **Random Forest model stands out clearly** as the most accurate model.  With **99.5% accuracy** and an **out-of-sample error rate of 0.5%** (calculated as [1-Accuracy]), we expect very little missclassification in the final predictions of the provided test set using the **Random Forest Model**.

## Final Predictions
```{r}
predict_final <- predict(model_rf, newdata=testing)
predict_final
```

